{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 02. Neural Network Classification with PyTorch (Excursus)\n",
    "In chapter 02 of the PyTorch course we trained a small neural network to classify a circular binary toy dataset (see `02_nn_classification.ipynb`). \n",
    "After \n",
    "training a two layer \n",
    "neural network\n",
    " with ReLU (for non-linearity) I became curious in how changing the hyperparameters would impact the performance and decision boundary of the model.\n",
    " \n",
    "This notebook summarizes my experimentation. Here is what I did:\n",
    "- increasing network depth\n",
    "- alternating the size of the hidden layers (encoding-decoding)\n",
    "- investigating causes and solutions to degrading performance of a five-layer neural network\n",
    "- detecting vanishing gradients\n",
    "- fixing vanishing gradients by decreasing the learning rate (five-layer neural network"
   ],
   "id": "e8b1a7b172356a97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:10:11.192907Z",
     "start_time": "2024-07-23T07:10:11.172784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_functions import plot_decision_boundary, get_data_loaders, fit, accuracy_fn, batch_loss\n",
    "torch.__version__"
   ],
   "id": "64d3f3dc52f7f54d",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T06:47:56.986289Z",
     "start_time": "2024-07-23T06:47:56.969864Z"
    }
   },
   "cell_type": "code",
   "source": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
   "id": "b0c22e29ed163f2f",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Recreating learning setup from chapter",
   "id": "ce08938b3e611e79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 Recreate dataset",
   "id": "5d7a695e2e6d8f6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-23T06:48:19.044430Z",
     "start_time": "2024-07-23T06:48:18.996585Z"
    }
   },
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Make 1000 samples\n",
    "n = 1000\n",
    "\n",
    "X, y = make_circles(n, noise=0.03, random_state=42)\n",
    "\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_ds = TensorDataset(X_train.to(device), y_train.to(device))\n",
    "test_ds = TensorDataset(X_test.to(device), y_test.to(device))\n",
    "\n",
    "train_dl, test_dl = get_data_loaders(train_ds, test_ds, batch_size=32)"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2. Recreate loss function",
   "id": "33aa6323d9bf0606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T06:49:48.287899Z",
     "start_time": "2024-07-23T06:49:48.284898Z"
    }
   },
   "cell_type": "code",
   "source": "loss_fn = nn.BCEWithLogitsLoss()",
   "id": "40d2790576df4221",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 Recreate two layer ReLU model",
   "id": "21861d77ff4d040c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T06:57:27.294520Z",
     "start_time": "2024-07-23T06:57:27.281322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model_relu = nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1)\n",
    ").to(device)\n",
    "\n",
    "optim_relu = torch.optim.SGD(params=model_relu.parameters(), lr=0.1)\n"
   ],
   "id": "2724d2afdf19bbc1",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T06:57:44.507759Z",
     "start_time": "2024-07-23T06:57:27.943829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fit(epochs=500, \n",
    "    model=model_relu, \n",
    "    loss_fn=loss_fn, \n",
    "    opt=optim_relu, \n",
    "    train_dl=train_dl, \n",
    "    test_dl=test_dl,\n",
    "    acc_fn=accuracy_fn)"
   ],
   "id": "7dddc74016e12394",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. More layers\n",
    "Let's investigate what happens when we add more layers to both of the networks."
   ],
   "id": "e74be8a0f78f6540"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1. Three layers",
   "id": "f8482e6fbc3c9bab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T06:58:10.975089Z",
     "start_time": "2024-07-23T06:58:10.953994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model_relu_3l = nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1)\n",
    ").to(device)\n",
    "\n",
    "optim_relu_3l = torch.optim.SGD(params=model_relu_3l.parameters(), lr=0.1)"
   ],
   "id": "2efbfe5c4433463e",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T06:58:28.830492Z",
     "start_time": "2024-07-23T06:58:11.611868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fit(epochs=500,\n",
    "    model=model_relu_3l,\n",
    "    loss_fn=loss_fn,\n",
    "    opt=optim_relu_3l,\n",
    "    train_dl=train_dl,\n",
    "    test_dl=test_dl,\n",
    "    acc_fn=accuracy_fn)"
   ],
   "id": "b772cddb31958f37",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's compare the decision boundaries of the ReLU model with two and with three layers!",
   "id": "ddf7839aa17a0cd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T06:58:32.390034Z",
     "start_time": "2024-07-23T06:58:32.036714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"ReLU model (two layers) on test data\")\n",
    "plot_decision_boundary(model_relu, X_test, y_test)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"ReLU model (three layers) on test data\")\n",
    "plot_decision_boundary(model_relu_3l, X_test, y_test)"
   ],
   "id": "9aef41c210e20616",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Adding another layer to the ReLU model results in a smoother and rounder decision boundary. Let's see if adding another layer increases performance.",
   "id": "fb1e67a27fc22aee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2. Four layers",
   "id": "7926e6064a14c18f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T06:59:17.364895Z",
     "start_time": "2024-07-23T06:59:17.343246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model_relu_4l = nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1)\n",
    ").to(device)\n",
    "\n",
    "optim_relu_4l = torch.optim.SGD(params=model_relu_4l.parameters(), lr=0.1)"
   ],
   "id": "9ca50c8bdfa10960",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T06:59:44.676907Z",
     "start_time": "2024-07-23T06:59:17.930989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fit(epochs=600,\n",
    "    model=model_relu_4l,\n",
    "    loss_fn=loss_fn,\n",
    "    opt=optim_relu_4l,\n",
    "    train_dl=train_dl,\n",
    "    test_dl=test_dl,\n",
    "    acc_fn=accuracy_fn)"
   ],
   "id": "21baee980a1f679d",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T06:59:57.743351Z",
     "start_time": "2024-07-23T06:59:57.321060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"ReLU model (three layers) on test data\")\n",
    "plot_decision_boundary(model_relu_3l, X_test, y_test)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"ReLU model (four layers) on test data\")\n",
    "plot_decision_boundary(model_relu_4l, X_test, y_test)\n"
   ],
   "id": "c1523af8e94229df",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hmmm, performance does not really increase. Instead, a network with four layers seems to do no better than a network with two layers. Let's try five.",
   "id": "dc91cd49c4eb9677"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3. Five layers",
   "id": "87aaf5437f06789"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:00:13.088522Z",
     "start_time": "2024-07-23T07:00:13.073288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model_relu_5l = nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1)\n",
    ").to(device)\n",
    "\n",
    "optim_relu_5l = torch.optim.SGD(params=model_relu_5l.parameters(), lr=0.1)"
   ],
   "id": "40f71ff84890ffa8",
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:00:52.593859Z",
     "start_time": "2024-07-23T07:00:25.905587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fit(epochs=600,\n",
    "    model=model_relu_5l,\n",
    "    loss_fn=loss_fn,\n",
    "    opt=optim_relu_5l,\n",
    "    train_dl=train_dl,\n",
    "    test_dl=test_dl,\n",
    "    acc_fn=accuracy_fn)"
   ],
   "id": "eba175dd91c92029",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:00:54.297131Z",
     "start_time": "2024-07-23T07:00:53.752676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"ReLU model (three layers) on test data\")\n",
    "plot_decision_boundary(model_relu_3l, X_test, y_test)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"ReLU model (five layers) on test data\")\n",
    "plot_decision_boundary(model_relu_5l, X_test, y_test)"
   ],
   "id": "1f1222844279012c",
   "execution_count": 36,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Interesting. I would have assumed that adding more hidden layers would results in the network learning more complex patterns, instead it results simply in a nonsensical performance decay.",
   "id": "6cd7b047a8bd14c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.4 Potential reasons for performance decay\n",
    "I asked chatGPT for potential reasons for the performance decay:\n",
    "\n",
    "1. **Model architecture**:\n",
    "The model has a uniform architecture where each hidden layer has 5 neurons. This can lead to redundant computations and insufficient learning capacity because the model might not be learning meaningful hierarchical representations.\n",
    "2. **Gradient Flow and Vanishing/Exploding Gradients**: In deeper networks, the gradients can vanish or explode during backpropagation, making it difficult for the network to learn effectively. The uniform architecture of the model might have exacerbated this issue.\n",
    "3. **Capacity and Overfitting**: The model might have too much capacity (due to the repeated layers with the same size), leading to overfitting on the noise in the data rather than learning the underlying patterns."
   ],
   "id": "ad567f4422647b2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3 Alternating hidden layer size in deep neural networks\n",
    "Let's try improve the model by tweaking the architecture. We could try to alternate the size of the hidden layers so that the model learns to encode and then reconstruct the underlying structure of the input data."
   ],
   "id": "54c7089509942c06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:05:21.481906Z",
     "start_time": "2024-07-23T07:05:21.448311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model_relu_5l_alt = nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, 1)\n",
    ").to(device)\n",
    "\n",
    "optim_relu_5l_alt = torch.optim.SGD(params=model_relu_5l_alt.parameters(), lr=0.1)"
   ],
   "id": "bf76b8443d70fbfa",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:06:00.004748Z",
     "start_time": "2024-07-23T07:05:32.448073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fit(epochs=600,\n",
    "    model=model_relu_5l_alt,\n",
    "    loss_fn=loss_fn,\n",
    "    opt=optim_relu_5l_alt,\n",
    "    train_dl=train_dl,\n",
    "    test_dl=test_dl,\n",
    "    acc_fn=accuracy_fn)"
   ],
   "id": "f499331e9b3f69ed",
   "execution_count": 38,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We see that it takes comparitively more time for the five layer neural network to converge, but with the hidden layers alternating in size, encoding and decoding the data, it seems to be able to perfectly learn the underlying structure of the data.",
   "id": "dcddda859edae180"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 Why does alternating hidden layer size increase performance?\n",
    "Here are some suggestions by chatGPT for the improved performance of the model:\n",
    "\n",
    "1. **Feature Abstraction**:\n",
    "This structure encourages the network to learn more abstract and hierarchical representations by progressively compressing and expanding the feature space, which can help in capturing complex patterns in the data.\n",
    "\n",
    "2. **Gradient Flow**: Alternating layer sizes help in stabilizing the gradient flow during backpropagation, reducing the likelihood of vanishing or exploding gradients, and leading to more effective weight updates.\n",
    "\n",
    "3. **Overfitting**:\n",
    "Provides a more balanced architecture that can help prevent overfitting by encouraging the network to focus on learning essential features rather than memorizing the training data.\n",
    "\n",
    "4. **Inductive bias**: Offers a better inductive bias for capturing the underlying structure of the data, especially for problems requiring complex decision boundaries."
   ],
   "id": "4b86752b766729bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "However, in the end the decision boundary that the five layer neural network learns does not look so different from the one that the three layer neural network learns.",
   "id": "8dd9355c3a68b79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:07:29.712800Z",
     "start_time": "2024-07-23T07:07:28.603353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"ReLU model (three layers) on test data\")\n",
    "plot_decision_boundary(model_relu_3l, X_test, y_test)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"ReLU model (five layers) with alternating sizes on test data\")\n",
    "plot_decision_boundary(model_relu_5l_alt, X_test, y_test)"
   ],
   "id": "f33fd2bae67c4a28",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Investigating performance decay in arbitrarily deep unifrom neural network\n",
    "\n",
    "Eventhough we have found a working architecture, it would still be intereseting to exlpore, why the unifrom five layer model couldn't come up with a decision boundary. We are scientists now.\n",
    "\n",
    "Let's see if we can validate chatGPT's suggestions and identify the underlying issue with the uniform five layer architecture."
   ],
   "id": "78688037066d5e05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1 Testing for vanishing gradients\n",
    "The first thing we can test is whether the gradients in the unifrom 5l nn vanish or explode. To do so, we only need to add some code to the fitting function, so that it also outputs the gradients of, say, the firsrt layer."
   ],
   "id": "51a156f796d9a9c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:10:18.751969Z",
     "start_time": "2024-07-23T07:10:18.739256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fit_output_grad(epochs, model, loss_fn, opt, train_dl, test_dl, acc_fn):\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "      batch_loss(model, loss_fn, acc_fn, xb, yb, opt)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "\n",
    "      model.eval()\n",
    "      with torch.inference_mode():\n",
    "        batch_losses, batch_acc, batch_sizes = zip(\n",
    "            *[batch_loss(model, loss_fn, acc_fn, xb, yb) for xb, yb in test_dl]\n",
    "        )\n",
    "        test_loss = np.sum((np.multiply(batch_losses, batch_sizes))) / np.sum(batch_sizes)\n",
    "        test_acc = np.sum((np.multiply(batch_acc, batch_sizes))) / np.sum(batch_sizes)\n",
    "      \n",
    "      print(f\"\"\"Epoch {epoch}| Loss: {test_loss:.3f}, Accuracy: {test_acc:.2f}\n",
    "            \n",
    "1st layer gradients: \n",
    "{model[0].weight.grad.mT.squeeze().detach().numpy()}\n",
    "            \n",
    "2nd layer gradients: \n",
    "{model[2].weight.grad.mT.squeeze().detach().numpy()}\n",
    "            \n",
    "3rd layer gradients: \n",
    "{model[4].weight.grad.mT.squeeze().detach().numpy()}\n",
    "            \n",
    "4th layer gradients: \n",
    "{model[6].weight.grad.mT.squeeze().detach().numpy()}\n",
    "            \n",
    "5th layer gradients: \n",
    "{model[8].weight.grad.mT.squeeze().detach().numpy()}\n",
    "      \n",
    "      \"\"\")"
   ],
   "id": "13543c1b7bc09547",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:10:26.842225Z",
     "start_time": "2024-07-23T07:10:26.823999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model_relu_5l = nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1)\n",
    ").to(device)\n",
    "\n",
    "optim_relu_5l = torch.optim.SGD(params=model_relu_5l.parameters(), lr=0.1)"
   ],
   "id": "3d5997e888b0fad6",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:11:00.917034Z",
     "start_time": "2024-07-23T07:10:37.606753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "fit_output_grad(epochs=500,\n",
    "    model=model_relu_5l,\n",
    "    loss_fn=loss_fn,\n",
    "    opt=optim_relu_5l,\n",
    "    train_dl=train_dl,\n",
    "    test_dl=test_dl,\n",
    "    acc_fn=accuracy_fn)"
   ],
   "id": "327c02cbc9c9c5b9",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From our little experiment we have found out that we are dealing with **vanishing gradients**. But why do they vanish, and how can we mitigate the problem? Let's ask chatGPT to give us some ideas to further look into.\n",
   "id": "4f566f0077d3a6f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5 Excursus: Vanishing gradients - causes and solutions\n",
    "\n",
    "### 5.1 Potential Reasons for Vanishing Gradients and Performance Degradation\n",
    "The following list was created by chatGPT.\n",
    "\n",
    "#### 1. Vanishing Gradients\n",
    "- **Explanation:** In deep networks, especially those with many layers, the gradients can become very small as they propagate back through the network during backpropagation. This issue is exacerbated by the use of certain activation functions like sigmoid or tanh, though ReLU can also experience this under certain conditions.\n",
    "- **Impact:** When gradients vanish, the network weights stop updating effectively, causing the network to stop learning and performance to stagnate or deteriorate.\n",
    "\n",
    "#### 2. Poor Initialization\n",
    "- **Explanation:** If the weights of the network are not initialized properly, it can lead to poor gradient flow. For instance, if weights are initialized with values that are too small, it can cause the gradients to vanish.\n",
    "- **Impact:** Poor initialization can lead to the network struggling to escape regions of the loss landscape with very small gradients, hindering learning.\n",
    "\n",
    "#### 3. Learning Rate Issues\n",
    "- **Explanation:** An inappropriate learning rate can cause learning dynamics issues. If the learning rate is too high, the model may initially learn patterns but then start diverging, leading to unstable training. Conversely, if the learning rate is too low, the model may fail to learn effectively.\n",
    "- **Impact:** An initially effective learning rate might cause the model to sporadically achieve good performance, but as training continues, the model might get stuck or diverge.\n",
    "\n",
    "#### 4. Network Capacity and Architecture\n",
    "- **Explanation:** The uniform network architecture with the same number of neurons in each layer might not provide the necessary capacity to capture complex patterns. This could lead to initial learning followed by overfitting or the network settling into poor local minima.\n",
    "- **Impact:** The network might initially find some useful patterns but then fail to generalize, leading to performance degradation.\n",
    "\n",
    "#### 5. Overfitting and Underfitting\n",
    "- **Explanation:** Overfitting occurs when the model learns noise or irrelevant details from the training data, which can lead to poor generalization to test data. Underfitting occurs when the model is too simple to capture the underlying patterns in the data.\n",
    "- **Impact:** The model might initially perform well on the training data but fail to maintain performance on unseen data, leading to an accuracy drop to 50% (random guessing).\n",
    "\n",
    "\n",
    "### 5.2 Possible Solutions\n",
    "The following list was created by chatGPT.\n",
    "\n",
    "#### 1. Better Weight Initialization\n",
    "Use initialization methods like Xavier (Glorot) or He initialization to ensure better gradient flow.\n",
    "```python\n",
    "nn.init.xavier_uniform_(layer.weight)\n",
    "```\n",
    "\n",
    "#### 2. Adjust Learning Rate\n",
    "Experiment with different learning rates or use learning rate schedules to adapt the learning rate during training.\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "```\n",
    "\n",
    "#### 3. Activation Functions\n",
    "Ensure the use of appropriate activation functions like ReLU, and consider using variants like Leaky ReLU or ELU to mitigate vanishing gradients.\n",
    "```python\n",
    "nn.LeakyReLU()\n",
    "```\n",
    "\n",
    "#### 4. Batch Normalization\n",
    "Incorporate batch normalization to stabilize the learning process and improve gradient flow.\n",
    "```python\n",
    "nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.BatchNorm1d(5),\n",
    "    nn.ReLU(),\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "#### 5. Regularization\n",
    "Apply dropout or weight decay to reduce overfitting and improve generalization.\n",
    "```python\n",
    "nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "#### 6. Gradient Clipping\n",
    "Use gradient clipping to prevent exploding gradients, which can also help stabilize training.\n",
    "```python\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "By addressing these potential issues, you can improve the stability and performance of your neural network, preventing the vanishing gradient problem and ensuring sustained learning throughout the training process."
   ],
   "id": "5d0f5783408b3e64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Modulating the learning rate\n",
    "Let's try to fix the vanishing gradients by decreasing the leaning rate of the five-layer uniform neural network."
   ],
   "id": "3e2a95aadbe4114b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:14:11.713814Z",
     "start_time": "2024-07-23T07:14:11.697639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model_relu_5l = nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1)\n",
    ").to(device)\n",
    "\n",
    "# use lr=0.01 instead of lr=0.1\n",
    "optim_relu_5l = torch.optim.SGD(params=model_relu_5l.parameters(), lr=0.01)"
   ],
   "id": "2449de7daa2ba8d9",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:15:14.030317Z",
     "start_time": "2024-07-23T07:14:19.095710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fit(epochs=1000,\n",
    "    model=model_relu_5l,\n",
    "    loss_fn=loss_fn,\n",
    "    opt=optim_relu_5l,\n",
    "    train_dl=train_dl,\n",
    "    test_dl=test_dl,\n",
    "    acc_fn=accuracy_fn)"
   ],
   "id": "b4b9c6a0f2240605",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Yay! It seems that we have found a solution to the issue. Apparently, a learning rate of 0.1 was to high for the five layer uniform neural network. Reducing the learning rate to 0.01 helped achieve an accuracy of 99% (previously unseen).\n",
    "\n",
    "Let's see if the accuracy holds up if we train the network for another 100 epochs or whether it decays again. As we have seen with the sporadic performance improvements when using lr=0.1. \n",
    "\n",
    "My bet is that we have reached a local minimum by using a smaller step size. In contrast, using a lr=0.1 the model overshot any local minima and landed on a shoulder in the optimization landscape."
   ],
   "id": "9e02520f82b506c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:15:36.026017Z",
     "start_time": "2024-07-23T07:15:29.408809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fit(epochs=100,\n",
    "    model=model_relu_5l,\n",
    "    loss_fn=loss_fn,\n",
    "    opt=optim_relu_5l,\n",
    "    train_dl=train_dl,\n",
    "    test_dl=test_dl,\n",
    "    acc_fn=accuracy_fn)"
   ],
   "id": "62d1398a1e73fecb",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model has indeed converged. Let's visualize the decision boundary the model has found!",
   "id": "9c7c2bbe952fc74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T07:15:52.433643Z",
     "start_time": "2024-07-23T07:15:51.489168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"ReLU, 3 uniform layers, lr=0.1\")\n",
    "plot_decision_boundary(model_relu_3l, X_test, y_test)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"ReLU, 5 alternating layers, lr=0.1\")\n",
    "plot_decision_boundary(model_relu_3l, X_test, y_test)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"ReLU, 5 uniform layer, lr=0.01\")\n",
    "plot_decision_boundary(model_relu_5l, X_test, y_test)"
   ],
   "id": "aa45b77f827ebd19",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Conclusion\n",
    "Our little experiment with increasing the depth of the neural network showed, that complex models do not necessarily do better. All the models learned some form of octagon which fitted the data well enough, but no model actually learned to represent the decision boundary as a clean circle (ground truth).\n",
    "\n",
    "In ou case, the use of ReLU inevitably leads to a jagged decision boundary. A smoother activation function like the sigmoid may have resulted in a \n",
    "rounder decision boundary resembling the true distribution (circle) more closely. However, ReLU is still able to classify the test data with 99% \n",
    "accuracy and has performance advantages over sigmoid.\n",
    "\n",
    "Although this is a simple toy dataset, our experimentation illustrates that creating an accurate model of reality involves more than adding more layers. It takes a deep understanding of the underlying problem and theoretical knowledge of machine learning."
   ],
   "id": "421abd4cef6d4681"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
